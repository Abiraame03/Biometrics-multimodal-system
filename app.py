# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16W5dVlPigjfL4584-ytIzC5pej51V3qO
"""

import streamlit as st
import cv2, mediapipe as mp, numpy as np, pickle, tempfile, requests, tensorflow as tf
from gtts import gTTS
import os

# ============================================================
# üîπ Load models from GitHub repository
# ============================================================
@st.cache_resource
def load_models():
    repo_url = "https://github.com/Abiraame03/Biometrics-multimodal-system/raw/main/gesture%20auth%20app%20models/"
    files = {
        "gesture_model": "gesture_model.tflite",
        "encoder": "gesture_label_encoder.pkl",
        "face": "face_embeddings.pkl",
        "iris": "iris_embeddings.pkl",
        "voice_map": "voice_map.pkl"
    }

    local_models = {}
    os.makedirs("models", exist_ok=True)
    for key, fname in files.items():
        url = repo_url + fname
        r = requests.get(url)
        if r.status_code == 200:
            with open(f"models/{fname}", "wb") as f:
                f.write(r.content)
            local_models[key] = f"models/{fname}"
        else:
            st.error(f"‚ùå Couldn't fetch {fname} from repo.")
    return local_models

models = load_models()

# ============================================================
# üîπ Load Gesture Model
# ============================================================
interpreter = tf.lite.Interpreter(model_path=models["gesture_model"])
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

encoder = pickle.load(open(models["encoder"], "rb"))
voice_map = pickle.load(open(models["voice_map"], "rb"))

mp_hands = mp.solutions.hands
mp_face = mp.solutions.face_mesh

st.title("üß† Multimodal Biometric System")
st.markdown("### ‚úã Real-time Gesture + üëÅÔ∏è Iris + üòä Emotion based Authentication")

mode = st.radio("Select Mode:", ["Gesture Authentication", "Face & Iris Recognition"])

# ============================================================
# ‚úã Gesture Authentication
# ============================================================
if mode == "Gesture Authentication":
    run = st.checkbox("Start Camera")
    if run:
        cap = cv2.VideoCapture(0)
        hands = mp_hands.Hands()
        stframe = st.empty()

        while run:
            ret, frame = cap.read()
            if not ret:
                st.warning("Camera not accessible.")
                break

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = hands.process(rgb)

            if result.multi_hand_landmarks:
                for hand in result.multi_hand_landmarks:
                    pts = np.array([[lm.x, lm.y, lm.z] for lm in hand.landmark]).flatten()
                    input_data = np.expand_dims(pts.astype(np.float32), axis=0)
                    interpreter.set_tensor(input_details[0]['index'], input_data)
                    interpreter.invoke()
                    output_data = interpreter.get_tensor(output_details[0]['index'])
                    gesture = encoder.inverse_transform([np.argmax(output_data)])[0]
                    cv2.putText(frame, f"{gesture}", (30, 50),
                                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,255,0), 3)

                    # üîä Speak the mapped voice
                    text = voice_map.get(gesture, "")
                    if text:
                        tts = gTTS(text)
                        tts.save("voice.mp3")
                        os.system("start voice.mp3" if os.name == "nt" else "afplay voice.mp3")

            stframe.image(frame, channels="BGR")

        cap.release()

# ============================================================
# üëÅÔ∏è Face + Iris Authentication
# ============================================================
else:
    uploaded_img = st.file_uploader("Upload a face image", type=["jpg", "png", "jpeg"])
    if uploaded_img is not None:
        img = np.frombuffer(uploaded_img.read(), np.uint8)
        img = cv2.imdecode(img, cv2.IMREAD_COLOR)
        st.image(img, caption="Uploaded Image", use_container_width=True)
        st.success("‚úÖ Face & Iris features matched successfully (Demo mode).")

st.markdown("---")
st.caption("Developed the Multimodal Biometric Authentication System")